{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Data Processing Pipeline\n",
    "\n",
    "This notebook executes the complete data preparation workflow for analyzing model attribution scores. It transforms raw experimental logs (JSON and `.dill` files) into a structured, aggregated, and filtered dataset suitable for comparative analysis.\n",
    "\n",
    "The pipeline processes the data in five sequential stages:\n",
    "\n",
    "1.  **Initialization**\n",
    "    Loads the raw experimental predictions from `solutions.json` and converts them into a long-format Pandas DataFrame, establishing the base structure for analysis.\n",
    "\n",
    "2.  **Chronological Sorting**\n",
    "    Organizes the text spans within each task chronologically (e.g., ensuring \"User Prompt\" appears before \"Action Step\") while preserving the original file order.\n",
    "\n",
    "3.  **Attribution Aggregation**\n",
    "    Iterates through the raw Inseq output files (`_inseq_out.dill`) to compute aggregated attribution scores (Mean, Max, Sum, AbsMax, etc.) for every defined text span (Plan and Action).\n",
    "\n",
    "4.  **Decoy Merging**\n",
    "    Consolidates split spans (specifically `decoy_functions_1` and `decoy_functions_2`) into single logical units, mathematically combining their attribution scores to create a unified \"Decoy Tools\" metric.\n",
    "\n",
    "5.  **Filtering & Normalization**\n",
    "    Performs final cleanup by filtering for tasks where data exists for *both* models (ensuring fair comparison) and reordering columns for readability and logical flow.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Input Data:** `data/solutions.json` and raw files in `outputs/`\n",
    "**Final Output:** `data/solutions_dataframe_normalized.parquet`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataframe to save solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def initialize_and_save_dataframe(json_filepath: str, output_filepath: str):\n",
    "    \"\"\"\n",
    "    Loads experimental data from a JSON file, structures it into a long-format\n",
    "    pandas DataFrame, and saves it to a Parquet file.\n",
    "\n",
    "    This version safely handles predictions that may be missing a 'spans' dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the entire JSON file into memory\n",
    "    with open(json_filepath, 'r') as f:\n",
    "        solutions_data = json.load(f)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    # --- Iterate through the nested JSON structure ---\n",
    "    for record in solutions_data:\n",
    "        task_id = str(record['id'])\n",
    "        category = record['category']\n",
    "\n",
    "        for prediction in record['predictions']:\n",
    "            model = prediction['model']\n",
    "            filename = prediction['file']\n",
    "\n",
    "            # 3. Inner loop: Iterate safely through spans, handling missing keys\n",
    "            for span_name, span_coords in prediction.get('spans', {}).items():\n",
    "                span_start, span_end = span_coords\n",
    "                span_len = span_end - span_start\n",
    "\n",
    "                # Create a dictionary representing a single row in our DataFrame\n",
    "                row = {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"model\": model,\n",
    "                    \"category\": category,\n",
    "                    \"filename\": filename,\n",
    "                    \"span_name\": span_name,\n",
    "                    \"span_start\": span_start,\n",
    "                    \"span_end\": span_end,\n",
    "                    \"span_len\": span_len,\n",
    "                    \"attr_plan\": np.nan,\n",
    "                    \"attr_action\": np.nan\n",
    "                }\n",
    "                all_rows.append(row)\n",
    "\n",
    "    # Create the DataFrame from the list of rows\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Reorder columns to match the desired final layout\n",
    "    column_order = [\n",
    "        \"task_id\", \"model\", \"category\", \"filename\", \n",
    "        \"span_name\", \"span_start\", \"span_end\", \"span_len\",\n",
    "        \"attr_plan\", \"attr_action\"\n",
    "    ]\n",
    "    df = df[column_order]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the DataFrame to a Parquet file for efficiency and type safety\n",
    "    df.to_parquet(output_filepath, index=False)\n",
    "    print(f\"DataFrame successfully created and saved to {output_filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_filepath = \"../data/solutions.json\"\n",
    "    output_filepath = \"../data/solutions_dataframe_new.parquet\"\n",
    "\n",
    "    initialize_and_save_dataframe(json_filepath, output_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort spans in dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsorted, before:      task_id       model category  \\\n",
      "5033    2100  Qwen3-0.6B     data   \n",
      "5034    2100  Qwen3-0.6B     data   \n",
      "5035    2100  Qwen3-0.6B     data   \n",
      "5036    2100  Qwen3-0.6B     data   \n",
      "5037    2100  Qwen3-0.6B     data   \n",
      "5038    2100  Qwen3-0.6B     data   \n",
      "5039    2100  Qwen3-0.6B     data   \n",
      "5040    2100  Qwen3-0.6B     data   \n",
      "5041    2100  Qwen3-0.6B     data   \n",
      "5042    2100  Qwen3-0.6B     data   \n",
      "5043    2100  Qwen3-0.6B     data   \n",
      "5044    2100  Qwen3-0.6B     data   \n",
      "\n",
      "                                               filename  is_correct  \\\n",
      "5033  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5034  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5035  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5036  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5037  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5038  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5039  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5040  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5041  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5042  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5043  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5044  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "\n",
      "                           span_name  span_start  span_end  span_len  \\\n",
      "5033  agentic_behaviour_instructions           0       211       211   \n",
      "5034               few_shot_examples         211      1465      1254   \n",
      "5035                 functions_intro        1465      1507        42   \n",
      "5036               final_answer_tool        3604      3643        39   \n",
      "5037          general_code_use_rules        3643      3990       347   \n",
      "5038                  call_to_action        3990      4019        29   \n",
      "5039                correct_function        1858      1907        49   \n",
      "5040               decoy_functions_1        1507      1858       351   \n",
      "5041               decoy_functions_2        1907      3604      1697   \n",
      "5042                     user_prompt        4019      4059        40   \n",
      "5043                   planning_step        4059      4943       884   \n",
      "5044                     action_step        4943      4972        29   \n",
      "\n",
      "      attr_plan  attr_action  \n",
      "5033        NaN          NaN  \n",
      "5034        NaN          NaN  \n",
      "5035        NaN          NaN  \n",
      "5036        NaN          NaN  \n",
      "5037        NaN          NaN  \n",
      "5038        NaN          NaN  \n",
      "5039        NaN          NaN  \n",
      "5040        NaN          NaN  \n",
      "5041        NaN          NaN  \n",
      "5042        NaN          NaN  \n",
      "5043        NaN          NaN  \n",
      "5044        NaN          NaN  \n",
      "Loading data from data/solutions_dataframe.parquet...\n",
      "Sorting spans within each filename group...\n",
      "Sorted DataFrame successfully saved to data/solutions_dataframe_sorted.parquet\n",
      "Unsorted, before:      task_id       model category  \\\n",
      "5033    2100  Qwen3-0.6B     data   \n",
      "5034    2100  Qwen3-0.6B     data   \n",
      "5035    2100  Qwen3-0.6B     data   \n",
      "5040    2100  Qwen3-0.6B     data   \n",
      "5039    2100  Qwen3-0.6B     data   \n",
      "5041    2100  Qwen3-0.6B     data   \n",
      "5036    2100  Qwen3-0.6B     data   \n",
      "5037    2100  Qwen3-0.6B     data   \n",
      "5038    2100  Qwen3-0.6B     data   \n",
      "5042    2100  Qwen3-0.6B     data   \n",
      "5043    2100  Qwen3-0.6B     data   \n",
      "5044    2100  Qwen3-0.6B     data   \n",
      "\n",
      "                                               filename  is_correct  \\\n",
      "5033  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5034  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5035  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5040  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5039  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5041  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5036  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5037  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5038  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5042  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5043  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "5044  data_ID2100_Qwen3-0.6B_21-09_10-24-45_agent_ou...       False   \n",
      "\n",
      "                           span_name  span_start  span_end  span_len  \\\n",
      "5033  agentic_behaviour_instructions           0       211       211   \n",
      "5034               few_shot_examples         211      1465      1254   \n",
      "5035                 functions_intro        1465      1507        42   \n",
      "5040               decoy_functions_1        1507      1858       351   \n",
      "5039                correct_function        1858      1907        49   \n",
      "5041               decoy_functions_2        1907      3604      1697   \n",
      "5036               final_answer_tool        3604      3643        39   \n",
      "5037          general_code_use_rules        3643      3990       347   \n",
      "5038                  call_to_action        3990      4019        29   \n",
      "5042                     user_prompt        4019      4059        40   \n",
      "5043                   planning_step        4059      4943       884   \n",
      "5044                     action_step        4943      4972        29   \n",
      "\n",
      "      attr_plan  attr_action  \n",
      "5033        NaN          NaN  \n",
      "5034        NaN          NaN  \n",
      "5035        NaN          NaN  \n",
      "5040        NaN          NaN  \n",
      "5039        NaN          NaN  \n",
      "5041        NaN          NaN  \n",
      "5036        NaN          NaN  \n",
      "5037        NaN          NaN  \n",
      "5038        NaN          NaN  \n",
      "5042        NaN          NaN  \n",
      "5043        NaN          NaN  \n",
      "5044        NaN          NaN  \n",
      "\n",
      "--- Verifying sort order for a sample file ---\n",
      "Original index vs. sorted 'span_start' for file: <pandas.core.indexing._iLocIndexer object at 0x131865220>\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [span_name, span_start, span_end]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def sort_spans_and_save(input_filepath: str, output_filepath: str):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame, sorts the spans within each filename group by their\n",
    "    start position, and saves the result to a new file.\n",
    "\n",
    "    The original order of the filename groups is preserved.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the source Parquet file.\n",
    "        output_filepath (str): The path to save the sorted Parquet file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_filepath):\n",
    "        print(f\"Error: Input file not found at {input_filepath}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading data from {input_filepath}...\")\n",
    "    dataframe = pd.read_parquet(input_filepath)\n",
    "\n",
    "    original_filename_order = dataframe['filename'].unique()\n",
    "    dataframe['filename'] = pd.Categorical(\n",
    "        dataframe['filename'],\n",
    "        categories=original_filename_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort by filename first (preserving original block order) and\n",
    "    # then by span_start to order the rows within each block.\n",
    "    print(\"Sorting spans within each filename group...\")\n",
    "    sorted_dataframe = dataframe.sort_values(by=['filename', 'span_start'])\n",
    "\n",
    "    # Convert the filename column back to a regular string object for saving\n",
    "    sorted_dataframe['filename'] = sorted_dataframe['filename'].astype(str)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the newly sorted DataFrame\n",
    "    sorted_dataframe.to_parquet(output_filepath, index=False)\n",
    "    print(f\"Sorted DataFrame successfully saved to {output_filepath}\")\n",
    "\n",
    "    return sorted_dataframe\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your input and output paths here\n",
    "    input_path = \"../data/solutions_dataframe_new.parquet\"\n",
    "    output_path_sorted = \"../data/solutions_dataframe_new.parquet\"\n",
    "\n",
    "    # Run the sorting function\n",
    "    sort_spans_and_save(input_path, output_path_sorted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Inseq Spans and save to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sorted DataFrame from data/solutions_dataframe_sorted.parquet...\n",
      "Preparing DataFrame with new attribution columns...\n",
      "Found 482 unique files to process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243fcfc0c63c4eb3baa09fd992d26c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating Files:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregation and data population complete.\n",
      "Saving final populated DataFrame to data/solutions_dataframe_final.parquet...\n",
      "\n",
      "--- Verification ---\n",
      "Final DataFrame Info (note the new columns):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5760 entries, 0 to 5759\n",
      "Data columns (total 25 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   task_id             5760 non-null   object \n",
      " 1   model               5760 non-null   object \n",
      " 2   category            5760 non-null   object \n",
      " 3   filename            5760 non-null   object \n",
      " 4   is_correct          5760 non-null   bool   \n",
      " 5   span_name           5760 non-null   object \n",
      " 6   span_start          5760 non-null   int64  \n",
      " 7   span_end            5760 non-null   int64  \n",
      " 8   span_len            5760 non-null   int64  \n",
      " 9   attr_plan           0 non-null      float64\n",
      " 10  attr_action         0 non-null      float64\n",
      " 11  absmax_plan_attr    5278 non-null   object \n",
      " 12  absmax_action_attr  5760 non-null   object \n",
      " 13  prod_plan_attr      4796 non-null   object \n",
      " 14  prod_action_attr    5278 non-null   object \n",
      " 15  min_plan_attr       4796 non-null   object \n",
      " 16  min_action_attr     5278 non-null   object \n",
      " 17  vnorm_plan_attr     4796 non-null   object \n",
      " 18  vnorm_action_attr   5278 non-null   object \n",
      " 19  mean_plan_attr      4796 non-null   object \n",
      " 20  mean_action_attr    5278 non-null   object \n",
      " 21  sum_plan_attr       4796 non-null   object \n",
      " 22  sum_action_attr     5278 non-null   object \n",
      " 23  max_plan_attr       4796 non-null   object \n",
      " 24  max_action_attr     5278 non-null   object \n",
      "dtypes: bool(1), float64(2), int64(3), object(19)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "First 5 rows of the final DataFrame:\n",
      "  task_id       model                   category  \\\n",
      "0    2078  Qwen3-1.7B  business_and_productivity   \n",
      "1    2078  Qwen3-1.7B  business_and_productivity   \n",
      "2    2078  Qwen3-1.7B  business_and_productivity   \n",
      "3    2078  Qwen3-1.7B  business_and_productivity   \n",
      "4    2078  Qwen3-1.7B  business_and_productivity   \n",
      "\n",
      "                                            filename  is_correct  \\\n",
      "0  business_and_productivity_ID2078_Qwen3-1.7B_22...        True   \n",
      "1  business_and_productivity_ID2078_Qwen3-1.7B_22...        True   \n",
      "2  business_and_productivity_ID2078_Qwen3-1.7B_22...        True   \n",
      "3  business_and_productivity_ID2078_Qwen3-1.7B_22...        True   \n",
      "4  business_and_productivity_ID2078_Qwen3-1.7B_22...        True   \n",
      "\n",
      "                        span_name  span_start  span_end  span_len  attr_plan  \\\n",
      "0  agentic_behaviour_instructions           0       211       211        NaN   \n",
      "1               few_shot_examples         211      1465      1254        NaN   \n",
      "2                 functions_intro        1465      1507        42        NaN   \n",
      "3               decoy_functions_1        1507      2426       919        NaN   \n",
      "4                correct_function        2426      2490        64        NaN   \n",
      "\n",
      "   ...  min_plan_attr min_action_attr vnorm_plan_attr vnorm_action_attr  \\\n",
      "0  ...       0.000031         0.00005        0.950356          0.421523   \n",
      "1  ...       0.000006        0.000009         0.17499           0.09026   \n",
      "2  ...       0.000021        0.000019        0.036591          0.006691   \n",
      "3  ...       0.000007         0.00001        0.099489          0.023157   \n",
      "4  ...       0.000012        0.000015        0.092095          0.027864   \n",
      "\n",
      "  mean_plan_attr mean_action_attr sum_plan_attr sum_action_attr max_plan_attr  \\\n",
      "0       0.000664         0.000869     35.025742        5.500116      0.192047   \n",
      "1        0.00019         0.000218     59.585094         8.20393      0.026677   \n",
      "2       0.000237         0.000112      2.488225        0.140577      0.008541   \n",
      "3       0.000129         0.000088     29.525894         2.42937      0.009443   \n",
      "4       0.000329         0.000252       5.27164        0.484789      0.020766   \n",
      "\n",
      "  max_action_attr  \n",
      "0        0.128871  \n",
      "1        0.024963  \n",
      "2        0.003127  \n",
      "3        0.003289  \n",
      "4        0.008217  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dill\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "from inseq.data.aggregator import AggregatorPipeline, ContiguousSpanAggregator\n",
    "\n",
    "def aggregate_and_populate(\n",
    "    sorted_dataframe: pd.DataFrame,\n",
    "    base_path: str = \"../outputs/\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs inseq span aggregation for every unique file in the DataFrame and\n",
    "    populates the DataFrame with the attribution scores.\n",
    "\n",
    "    Args:\n",
    "        sorted_dataframe (pd.DataFrame): The pre-sorted DataFrame from the previous step.\n",
    "        base_path (str): The root directory where inseq output folders are located.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame, now populated with attribution scores.\n",
    "    \"\"\"\n",
    "    # List of methods aggregation methods\n",
    "    methods = ['absmax', 'prod', 'min', 'vnorm', 'mean', 'sum', 'max']\n",
    "\n",
    "    # --- 1. Prepare the DataFrame by adding new columns ---\n",
    "    print(\"Preparing DataFrame with new attribution columns...\")\n",
    "    for method in methods:\n",
    "        sorted_dataframe[f\"{method}_plan_attr\"] = pd.NA\n",
    "        sorted_dataframe[f\"{method}_action_attr\"] = pd.NA\n",
    "\n",
    "    # Get a list of unique filenames to iterate over\n",
    "    unique_files = sorted_dataframe['filename'].unique()\n",
    "    print(f\"Found {len(unique_files)} unique files to process.\")\n",
    "\n",
    "    # --- 2. Main loop: Iterate through each unique file ---\n",
    "    for filename in tqdm(unique_files, desc=\"Aggregating Files\"):\n",
    "        # Get all rows corresponding to the current file\n",
    "        file_specific_rows = sorted_dataframe[sorted_dataframe['filename'] == filename]\n",
    "        \n",
    "        # --- 3. Construct the path and load the inseq attribution file ---\n",
    "        category = file_specific_rows['category'].iloc[0]\n",
    "        inseq_filename = filename.replace(\"_agent_out.dill\", \"_inseq_out.dill\")\n",
    "        inseq_filepath = os.path.join(base_path, category, inseq_filename)\n",
    "\n",
    "        try:\n",
    "            with open(inseq_filepath, 'rb') as f:\n",
    "                loaded_attributions = dill.load(f)\n",
    "        except (FileNotFoundError, dill.UnpicklingError) as e:\n",
    "            print(f\"\\nWarning: Could not load or parse {inseq_filepath}. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- 4. Prepare spans and names for the aggregator ---\n",
    "        target_spans = list(zip(\n",
    "            file_specific_rows['span_start'],\n",
    "            file_specific_rows['span_end']\n",
    "        ))\n",
    "        target_names = file_specific_rows['span_name'].tolist()\n",
    "\n",
    "        # --- 5. Inner loop: Run aggregation for each method ---\n",
    "        for method in methods:\n",
    "            # Initialize the pipeline with the current method\n",
    "            span_pipeline = AggregatorPipeline(\n",
    "               [ContiguousSpanAggregator],\n",
    "               [method]\n",
    "            )\n",
    "\n",
    "            # Aggregate the attributions\n",
    "            token_aggregated = loaded_attributions.sequence_attributions[0].aggregate(\n",
    "                aggregator=span_pipeline,\n",
    "                target_spans=target_spans\n",
    "            )\n",
    "            \n",
    "            # Name the aggregated spans\n",
    "            for i, name in enumerate(target_names):\n",
    "                token_aggregated.target[i].token = name\n",
    "\n",
    "            # --- 6. Extract results and save to the main DataFrame ---\n",
    "            # The result is a tensor with shape [num_spans, 2]\n",
    "            attr_tensor = token_aggregated.target_attributions\n",
    "\n",
    "            # Get the indices in the main DataFrame where we need to save the data\n",
    "            target_indices = file_specific_rows.index\n",
    "\n",
    "            # Define the column names for the current method\n",
    "            plan_col = f\"{method}_plan_attr\"\n",
    "            action_col = f\"{method}_action_attr\"\n",
    "\n",
    "            # Update the DataFrame using.loc for safe and efficient assignment\n",
    "            sorted_dataframe.loc[target_indices, plan_col] = attr_tensor[:, 0].tolist()\n",
    "            sorted_dataframe.loc[target_indices, action_col] = attr_tensor[:, 1].tolist()\n",
    "\n",
    "    print(\"\\nAggregation and data population complete.\")\n",
    "    return sorted_dataframe\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load the sorted DataFrame from the previous step\n",
    "    input_path = \"../data/solutions_dataframe_new.parquet\"\n",
    "    print(f\"Loading sorted DataFrame from {input_path}...\")\n",
    "    df_sorted = pd.read_parquet(input_path)\n",
    "\n",
    "    # 2. Run the main aggregation function\n",
    "    df_final = aggregate_and_populate(df_sorted)\n",
    "\n",
    "    # 3. Save the final, populated DataFrame\n",
    "    output_path_final = \"../data/solutions_dataframe_aggregated.parquet\"\n",
    "    print(f\"Saving final populated DataFrame to {output_path_final}...\")\n",
    "    df_final.to_parquet(output_path_final, index=False)\n",
    "\n",
    "    # 4. Verify the result\n",
    "    print(\"\\n--- Verification ---\")\n",
    "    print(\"Final DataFrame Info (note the new columns):\")\n",
    "    df_final.info()\n",
    "    print(\"\\nFirst 5 rows of the final DataFrame:\")\n",
    "    print(df_final.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataframe and combine \"decoy_function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataFrame from data/solutions_dataframe_final.parquet...\n",
      "DataFrame loaded successfully.\n",
      "Column 'attr_plan' exists and is empty. Deleting it.\n",
      "Column 'attr_action' exists and is empty. Deleting it.\n",
      "\n",
      "Starting aggregation of 'decoy_functions' spans...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a218f46efbb741b4bb8a43e6ad8dfd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rebuilding final DataFrame from processed rows...\n",
      "Aggregation complete. New DataFrame saved to data/solutions_dataframe_aggregated.parquet\n",
      "\n",
      "--- Verification ---\n",
      "Shape of new DataFrame: (5302, 23)\n",
      "Span names in the new DataFrame: ['agentic_behaviour_instructions' 'few_shot_examples' 'functions_intro'\n",
      " 'correct_function' 'final_answer_tool' 'general_code_use_rules'\n",
      " 'call_to_action' 'user_prompt' 'planning_step' 'action_step'\n",
      " 'decoy_functions']\n",
      "\n",
      "Checking spans for sample file: business_and_productivity_ID1001_Qwen3-0.6B_17-09_16-19-14_agent_out.dill\n",
      "                         span_name  span_len  sum_plan_attr\n",
      "0   agentic_behaviour_instructions       211      29.680502\n",
      "1                few_shot_examples      1254      47.577736\n",
      "2                  functions_intro        42       2.117552\n",
      "3                 correct_function        75       8.512258\n",
      "4                final_answer_tool        39       2.686046\n",
      "5           general_code_use_rules       347      30.701908\n",
      "6                   call_to_action        29       4.082032\n",
      "7                      user_prompt        39      19.518015\n",
      "8                    planning_step       244            NaN\n",
      "9                      action_step        36            NaN\n",
      "10                 decoy_functions      2003      42.687214\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Part 1: Aggregation Logic Helper Function ---\n",
    "\n",
    "def combine_values(val1: float, val2: float, len1: int, len2: int, method: str) -> float:\n",
    "    \"\"\"\n",
    "    Combines two attribution values using a specified aggregation method.\n",
    "\n",
    "    Args:\n",
    "        val1 (float): The value from the first span.\n",
    "        val2 (float): The value from the second span.\n",
    "        len1 (int): The length of the first span.\n",
    "        len2 (int): The length of the second span.\n",
    "        method (str): The aggregation method name.\n",
    "\n",
    "    Returns:\n",
    "        float: The combined value.\n",
    "    \"\"\"\n",
    "    if method == 'sum':\n",
    "        return val1 + val2\n",
    "    elif method == 'mean':\n",
    "        total_length = len1 + len2\n",
    "        if total_length == 0: return 0\n",
    "        weighted_sum = (val1 * len1) + (val2 * len2)\n",
    "        return weighted_sum / total_length\n",
    "    elif method == 'max':\n",
    "        return max(val1, val2)\n",
    "    elif method == 'min':\n",
    "        return min(val1, val2)\n",
    "    elif method == 'prod':\n",
    "        return val1 * val2\n",
    "    elif method == 'absmax':\n",
    "        return val1 if abs(val1) >= abs(val2) else val2\n",
    "    elif method == 'vnorm':  # Assumes L2 norm\n",
    "        return math.sqrt(val1**2 + val2**2)\n",
    "    else:\n",
    "        # Fallback for any unexpected method, though this shouldn't be hit\n",
    "        return np.nan\n",
    "\n",
    "# --- Part 2: Main Script to Load, Clean, and Aggregate ---\n",
    "\n",
    "# 1. Load the DataFrame\n",
    "final_filepath = \"../data/solutions_dataframe_aggregated.parquet\"\n",
    "print(f\"Loading DataFrame from {final_filepath}...\")\n",
    "df = pd.read_parquet(final_filepath)\n",
    "print(\"DataFrame loaded successfully.\")\n",
    "\n",
    "# 2. Delete initial placeholder columns if they are empty\n",
    "cols_to_delete = [\"attr_plan\", \"attr_action\"]\n",
    "for col in cols_to_delete:\n",
    "    if col in df.columns and df[col].isnull().all():\n",
    "        print(f\"Column '{col}' exists and is empty. Deleting it.\")\n",
    "        df = df.drop(columns=[col])\n",
    "    else:\n",
    "        print(f\"Column '{col}' does not exist or is not empty. No action taken.\")\n",
    "\n",
    "# 3. Prepare for aggregation\n",
    "print(\"\\nStarting aggregation of 'decoy_functions' spans...\")\n",
    "aggregation_methods = ['absmax', 'prod', 'min', 'vnorm', 'mean', 'sum', 'max']\n",
    "processed_rows = []\n",
    "\n",
    "# Group by 'filename' to process each file's spans independently\n",
    "grouped = df.groupby('filename')\n",
    "\n",
    "for filename, group_df in tqdm(grouped, desc=\"Processing files\"):\n",
    "    # Isolate the decoy spans and all other spans\n",
    "    decoy1 = group_df[group_df['span_name'] == 'decoy_functions_1']\n",
    "    decoy2 = group_df[group_df['span_name'] == 'decoy_functions_2']\n",
    "    other_rows = group_df[~group_df['span_name'].isin(['decoy_functions_1', 'decoy_functions_2'])]\n",
    "\n",
    "    # This list will hold the new/modified rows for this group\n",
    "    current_group_new_rows = [other_rows]\n",
    "\n",
    "    # Case 1: Both decoy functions exist, so we must combine them\n",
    "    if not decoy1.empty and not decoy2.empty:\n",
    "        d1_row = decoy1.iloc[0]\n",
    "        d2_row = decoy2.iloc[0]\n",
    "\n",
    "        # Start building the new combined row\n",
    "        new_row = {\n",
    "            \"task_id\": d1_row['task_id'],\n",
    "            \"model\": d1_row['model'],\n",
    "            \"category\": d1_row['category'],\n",
    "            \"filename\": d1_row['filename'],\n",
    "            \"span_name\": \"decoy_functions\",\n",
    "            \"span_start\": np.nan,\n",
    "            \"span_end\": np.nan,\n",
    "            \"span_len\": d1_row['span_len'] + d2_row['span_len']\n",
    "        }\n",
    "\n",
    "        # Apply the combination algorithm for each attribution method\n",
    "        for method in aggregation_methods:\n",
    "            # Combine the 'plan' attributes\n",
    "            new_row[f'{method}_plan_attr'] = combine_values(\n",
    "                d1_row[f'{method}_plan_attr'], d2_row[f'{method}_plan_attr'],\n",
    "                d1_row['span_len'], d2_row['span_len'], method\n",
    "            )\n",
    "            # Combine the 'action' attributes\n",
    "            new_row[f'{method}_action_attr'] = combine_values(\n",
    "                d1_row[f'{method}_action_attr'], d2_row[f'{method}_action_attr'],\n",
    "                d1_row['span_len'], d2_row['span_len'], method\n",
    "            )\n",
    "        \n",
    "        current_group_new_rows.append(pd.DataFrame([new_row]))\n",
    "\n",
    "    # Case 2: Only one of the decoy functions exists, so we just rename it\n",
    "    elif not decoy1.empty:\n",
    "        renamed_decoy = decoy1.copy()\n",
    "        renamed_decoy['span_name'] = 'decoy_functions'\n",
    "        current_group_new_rows.append(renamed_decoy)\n",
    "    elif not decoy2.empty:\n",
    "        renamed_decoy = decoy2.copy()\n",
    "        renamed_decoy['span_name'] = 'decoy_functions'\n",
    "        current_group_new_rows.append(renamed_decoy)\n",
    "\n",
    "    # Add the processed rows for this group to our main list\n",
    "    processed_rows.append(pd.concat(current_group_new_rows))\n",
    "\n",
    "# 4. Rebuild the final DataFrame\n",
    "print(\"\\nRebuilding final DataFrame from processed rows...\")\n",
    "final_aggregated_df = pd.concat(processed_rows, ignore_index=True)\n",
    "\n",
    "# 5. Save the new DataFrame\n",
    "output_path = \"../data/solutions_dataframe_aggregated.parquet\"\n",
    "final_aggregated_df.to_parquet(output_path, index=False)\n",
    "print(f\"Aggregation complete. New DataFrame saved to {output_path}\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(\"Shape of new DataFrame:\", final_aggregated_df.shape)\n",
    "print(\"Span names in the new DataFrame:\", final_aggregated_df['span_name'].unique())\n",
    "\n",
    "# Check a sample file to see the result\n",
    "sample_filename = final_aggregated_df['filename'].iloc[0]\n",
    "print(f\"\\nChecking spans for sample file: {sample_filename}\")\n",
    "print(final_aggregated_df[final_aggregated_df['filename'] == sample_filename][['span_name', 'span_len', 'sum_plan_attr']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Task IDs that dont have both models. Reorder methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aggregated DataFrame from data/solutions_dataframe_aggregated.parquet...\n",
      "\n",
      "--- Original DataFrame Info (Before Filtering) ---\n",
      "Number of rows: 5302\n",
      "Number of columns: 23\n",
      "Column names: ['task_id', 'model', 'category', 'filename', 'is_correct', 'span_name', 'span_start', 'span_end', 'span_len', 'absmax_plan_attr', 'absmax_action_attr', 'prod_plan_attr', 'prod_action_attr', 'min_plan_attr', 'min_action_attr', 'vnorm_plan_attr', 'vnorm_action_attr', 'mean_plan_attr', 'mean_action_attr', 'sum_plan_attr', 'sum_action_attr', 'max_plan_attr', 'max_action_attr']\n",
      "\n",
      "Number of unique 'task_id' entries: 250\n",
      "Average number of unique 'model' per 'task_id': 1.93\n",
      "Average number of 'span_name' per 'filename': 11.00\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Found 232 task IDs with data for both models.\n",
      "\n",
      "--- Filtered DataFrame Info (Tasks with Both Models Only) ---\n",
      "Number of rows: 5104\n",
      "Number of columns: 23\n",
      "Column names: ['task_id', 'model', 'category', 'filename', 'is_correct', 'span_name', 'span_start', 'span_end', 'span_len', 'absmax_plan_attr', 'absmax_action_attr', 'prod_plan_attr', 'prod_action_attr', 'min_plan_attr', 'min_action_attr', 'vnorm_plan_attr', 'vnorm_action_attr', 'mean_plan_attr', 'mean_action_attr', 'sum_plan_attr', 'sum_action_attr', 'max_plan_attr', 'max_action_attr']\n",
      "\n",
      "Number of unique 'task_id' entries: 232\n",
      "Average number of unique 'model' per 'task_id': 2.00\n",
      "Average number of 'span_name' per 'filename': 11.00\n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Reordering Columns ---\n",
      "\n",
      "Original column order:\n",
      "['task_id', 'model', 'category', 'filename', 'is_correct', 'span_name', 'span_start', 'span_end', 'span_len', 'absmax_plan_attr', 'absmax_action_attr', 'prod_plan_attr', 'prod_action_attr', 'min_plan_attr', 'min_action_attr', 'vnorm_plan_attr', 'vnorm_action_attr', 'mean_plan_attr', 'mean_action_attr', 'sum_plan_attr', 'sum_action_attr', 'max_plan_attr', 'max_action_attr']\n",
      "\n",
      "New column order:\n",
      "['task_id', 'model', 'category', 'filename', 'is_correct', 'span_name', 'span_start', 'span_end', 'span_len', 'vnorm_plan_attr', 'vnorm_action_attr', 'mean_plan_attr', 'mean_action_attr', 'prod_plan_attr', 'prod_action_attr', 'absmax_plan_attr', 'absmax_action_attr', 'max_plan_attr', 'max_action_attr', 'min_plan_attr', 'min_action_attr', 'sum_plan_attr', 'sum_action_attr']\n",
      "--------------------------\n",
      "\n",
      "✅ Filtered and reordered DataFrame saved successfully to: data/solutions_dataframe_normalized.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def print_dataframe_info(df: pd.DataFrame, title: str):\n",
    "    \"\"\"\n",
    "    Prints a detailed summary of a DataFrame's properties.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    \n",
    "    # Basic info\n",
    "    num_rows, num_cols = df.shape\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Number of columns: {num_cols}\")\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    \n",
    "    # Uniqueness stats\n",
    "    unique_tasks = df['task_id'].nunique()\n",
    "    print(f\"\\nNumber of unique 'task_id' entries: {unique_tasks}\")\n",
    "    \n",
    "    # Calculated averages\n",
    "    if not df.empty:\n",
    "        avg_models_per_task = df.groupby('task_id')['model'].nunique().mean()\n",
    "        avg_spans_per_file = df.groupby('filename')['span_name'].nunique().mean()\n",
    "        print(f\"Average number of unique 'model' per 'task_id': {avg_models_per_task:.2f}\")\n",
    "        print(f\"Average number of 'span_name' per 'filename': {avg_spans_per_file:.2f}\")\n",
    "    \n",
    "    print(\"-\" * (len(title) + 8))\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# 1. Load the aggregated DataFrame\n",
    "aggregated_filepath = \"data/solutions_dataframe_aggregated.parquet\"\n",
    "print(f\"Loading aggregated DataFrame from {aggregated_filepath}...\")\n",
    "df_agg = pd.read_parquet(aggregated_filepath)\n",
    "\n",
    "# 2. Print info for the original, unfiltered DataFrame\n",
    "print_dataframe_info(df_agg, \"Original DataFrame Info (Before Filtering)\")\n",
    "\n",
    "# 3. Filter for tasks that have entries for two different models\n",
    "model_counts_per_task = df_agg.groupby('task_id')['model'].nunique()\n",
    "tasks_with_both_models = model_counts_per_task[model_counts_per_task == 2].index\n",
    "df_filtered = df_agg[df_agg['task_id'].isin(tasks_with_both_models)].copy()\n",
    "\n",
    "# 4. Display the number of task IDs that have both models\n",
    "num_tasks_with_both = len(tasks_with_both_models)\n",
    "print(f\"\\n✅ Found {num_tasks_with_both} task IDs with data for both models.\")\n",
    "\n",
    "# 5. Print info for the newly filtered DataFrame\n",
    "print_dataframe_info(df_filtered, \"Filtered DataFrame Info (Tasks with Both Models Only)\")\n",
    "\n",
    "# 6. Reorder the columns\n",
    "print(\"\\n--- Reordering Columns ---\")\n",
    "print(\"\\nOriginal column order:\")\n",
    "print(df_filtered.columns.tolist())\n",
    "\n",
    "# Define the desired order for the attribution columns\n",
    "ordered_attr_cols = [\n",
    "    'vnorm_plan_attr', 'vnorm_action_attr', 'mean_plan_attr', 'mean_action_attr',\n",
    "    'prod_plan_attr', 'prod_action_attr', 'absmax_plan_attr', 'absmax_action_attr',\n",
    "    'max_plan_attr', 'max_action_attr', 'min_plan_attr', 'min_action_attr',\n",
    "    'sum_plan_attr', 'sum_action_attr'\n",
    "]\n",
    "\n",
    "# Get the list of all other columns, preserving their original order\n",
    "front_cols = [col for col in df_filtered.columns if col not in ordered_attr_cols]\n",
    "\n",
    "# Combine the two lists to create the final column order\n",
    "new_column_order = front_cols + ordered_attr_cols\n",
    "\n",
    "# Apply the new order to the DataFrame\n",
    "df_normalized = df_filtered[new_column_order]\n",
    "\n",
    "print(\"\\nNew column order:\")\n",
    "print(df_normalized.columns.tolist())\n",
    "print(\"--------------------------\")\n",
    "\n",
    "# 7. Save the new, normalized DataFrame\n",
    "output_dir = \"../data/\"\n",
    "output_filename = \"solutions_dataframe_normalized.parquet\"\n",
    "output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df_normalized.to_parquet(output_filepath, index=False)\n",
    "\n",
    "print(f\"\\n✅ Filtered and reordered DataFrame saved successfully to: {output_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10 (v3.12.10:0cc81280367, Apr  8 2025, 08:46:59) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bfb36e1809ebaabb37e5aa29ecba9279008a69c1743045940bd5d510f792dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
