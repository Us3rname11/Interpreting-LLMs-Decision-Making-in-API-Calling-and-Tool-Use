"""
Semantic Token Segmentation & Span Validation
=============================================

Purpose:
    Acts as a "Semantic Segmenter" for the raw token streams generated by the agent.
    It takes the flat list of tokens from an `inseq` output and divides them into 
    distinct, meaningful blocks (spans) without gaps.

    It identifies 4 groups of spans:
    1. **Static Prompts:** Fixed instructions (e.g., "agentic_behaviour_instructions").
    2. **Dynamic Prompts:** Text loaded from JSON (e.g., "final_answer_tool").
    3. **Function Definitions:** Locates the specific "Correct Tool" using regex 
       and defines "Decoy Tools" as the space surrounding it.
    4. **Agent Interaction:** Separates "User Prompt" from the model's response, 
       and splits the response into "Planning Step" (reasoning) and "Action Step" (code).

Validation:
    Performs a strict mathematical check to ensure that the identified spans cover 
    indices [0, Total_Tokens] contiguously, with no overlaps and no missing tokens.

Input:
    - Inseq Files: outputs/{category}/*_inseq_out.dill (Raw tokens)
    - Agent Files: outputs/{category}/*_agent_out.dill (Raw text for splitting thought/action)
    - Metadata: data/prompt_spans.json, data/solutions.json

Output:
    - Updates `data/solutions.json`: Adds a "spans" dictionary to each prediction record containing 
      [start, end] indices for every semantic block.
"""

import argparse
import collections
import json
import re
import dill
from pathlib import Path
from typing import List, Dict, Tuple, Optional, NamedTuple

class TokenWithId(NamedTuple):
    token: str
    id: int

def find_token_sequence(tokens: List[TokenWithId], search_text: str, search_space: Optional[Tuple[int, int]] = None) -> Optional[Tuple[int, int]]:
    """Finds the start and end indices of a search_text within tokens."""
    if not tokens or not search_text: return None
    start_offset = 0
    if search_space:
        start_offset, end_limit = search_space
        tokens = tokens[start_offset:end_limit]
    def clean_token(token_str: str) -> str: return token_str.replace('Ġ', ' ').replace('Ċ', '\n')
    cleaned_tokens = [clean_token(t.token) for t in tokens]
    for i in range(len(tokens)):
        current_match_string = ""
        for j in range(i, len(tokens)):
            current_match_string += cleaned_tokens[j]
            if current_match_string.startswith(search_text):
                return (i + start_offset, j + 1 + start_offset)
            if not search_text.startswith(current_match_string):
                break
    return None

# --- Helper Functions for Each Group ---

def _get_predefined_spans(prompt_spans_data: list) -> Dict[str, Tuple[int, int]]:
    spans = {}
    names_to_get = ["agentic_behaviour_instructions", "few_shot_examples", "functions_intro"]
    for item in prompt_spans_data:
        if item["name"] in names_to_get:
            spans[item["name"]] = tuple(item["span_tuple"])
    return spans

def _find_spans_from_json_content(tokens: List[TokenWithId], prompt_spans_data: list) -> Dict[str, Tuple[int, int]]:
    spans = {}
    names_to_get = ["final_answer_tool", "general_code_use_rules", "call_to_action"]
    name_to_content = {item['name']: item['content'] for item in prompt_spans_data}
    for name in names_to_get:
        content = name_to_content.get(name)
        if content:
            span = find_token_sequence(tokens, content)
            if span: spans[name] = span
    return spans

def _find_function_spans(tokens: List[TokenWithId], inseq_filepath: Path, solutions_data: list, known_spans: dict) -> Dict[str, Tuple[int, int]]:
    spans = {}
    functions_intro_span = known_spans.get("functions_intro")
    final_answer_tool_span = known_spans.get("final_answer_tool")
    if not all([functions_intro_span, final_answer_tool_span]): return {}
    match = re.search(r'_ID(\d{4})_', inseq_filepath.name)
    if not match: return {}
    task_id = int(match.group(1))
    task_name = next((item['task'] for item in solutions_data if item['id'] == task_id), None)
    if not task_name: return {}
    start_search_string = f"def {task_name}"
    function_search_space = (functions_intro_span[1], final_answer_tool_span[0])
    start_span_info = find_token_sequence(tokens, start_search_string, search_space=function_search_space)
    if not start_span_info: return {}
    start_index = start_span_info[0]
    end_index = -1
    def clean_token(token_str: str) -> str: return token_str.replace('Ġ', ' ').replace('Ċ', '\n')
    for j in range(start_index, function_search_space[1]):
        current_text = "".join([clean_token(t.token) for t in tokens[start_index : j + 1]])
        if current_text.count('"""') >= 2:
            end_index = j + 1
            break
    if end_index == -1: return {}
    correct_function_span = (start_index, end_index)
    spans["correct_function"] = correct_function_span
    decoy1_span = (functions_intro_span[1], correct_function_span[0])
    spans["decoy_functions_1"] = decoy1_span if decoy1_span[0] < decoy1_span[1] else None
    decoy2_span = (correct_function_span[1], final_answer_tool_span[0])
    spans["decoy_functions_2"] = decoy2_span if decoy2_span[0] < decoy2_span[1] else None
    return spans

def _find_agent_interaction_spans(tokens: List[TokenWithId], agent_out_filepath: Path) -> Dict[str, Tuple[int, int]]:
    """Group 4: Finds user_prompt, planning, and action steps."""
    spans = {}
    
    # Logic for user_prompt remains the same
    start_marker, end_marker = "<|im_start|>user\n", "<|im_end|>\n"
    start_span = find_token_sequence(tokens, start_marker)
    if start_span:
        end_span = find_token_sequence(tokens, end_marker, search_space=(start_span[0], len(tokens)))
        if end_span: spans["user_prompt"] = (start_span[0], end_span[1])

    # Logic for planning_step and action_step is updated below
    if not agent_out_filepath.exists():
        # This part is unchanged
        return spans 
        
    with open(agent_out_filepath, 'rb') as f:
        loaded_agent_out = dill.load(f)

    combined_content = loaded_agent_out[1].get("model_output_message", {}).get("content", "N/A")
    
    last_think_pos = combined_content.rfind("</think>")
    if last_think_pos != -1:
        
        # Define a variable for the split position between planning and action
        split_point = -1

        # First, try to find the <code> tag after the last </think> (primary logic)
        code_start_pos = combined_content.find("<code>", last_think_pos)
        
        if code_start_pos != -1:
            # If <code> is found, use its starting position as the split point
            split_point = code_start_pos
        else:
            # FALLBACK LOGIC: If <code> is NOT found, use the end of the </think> tag
            # as the split point. This assumes the action step starts immediately after.
            print(f"  - Info: No `<code>` tag found in {agent_out_filepath.name}. Using fallback for action_step.")
            split_point = last_think_pos + len("</think>")

        # Use the determined split_point to define the planning string.
        # The planning_str is everything from the start of the content to the split_point.
        planning_str = combined_content[:split_point]
            
        full_planning_str = f"<|im_start|>assistant\n{planning_str}"
        planning_span = find_token_sequence(tokens, full_planning_str)

        if planning_span:
            spans["planning_step"] = planning_span
            spans["action_step"] = (planning_span[1], len(tokens))

    return spans

def _validate_spans(all_spans: dict, total_tokens: int) -> Tuple[bool, str]:
    """Checks for contiguity and returns a status and detailed message."""
    valid_spans = sorted([s for s in all_spans.values() if s is not None], key=lambda x: x[0])
    if not valid_spans:
        return False, "Validation failed: No valid spans were found."
    if valid_spans[0][0] != 0:
        return False, f"Spans do not start at token 0 (start at {valid_spans[0][0]})."
    if valid_spans[-1][1] != total_tokens:
        return False, f"Spans do not end at last token (end at {valid_spans[-1][1]}, expected {total_tokens})."
    for i in range(len(valid_spans) - 1):
        if valid_spans[i][1] != valid_spans[i+1][0]:
            return False, f"Gap/overlap between spans. One ends at {valid_spans[i][1]}, next starts at {valid_spans[i+1][0]}."
    return True, "Success"

# --- Main Orchestrator Function ---

def get_all_spans_from_file(inseq_filepath: Path, prompt_spans_data: list, solutions_data: list) -> Tuple[Optional[Dict], bool, str]:
    """Returns spans, validation status, and validation message."""
    try:
        with open(inseq_filepath, 'rb') as f: loaded_inseq_data = dill.load(f)
    except (IOError, dill.UnpicklingError) as e:
        return None, False, f"Error loading dill file: {e}"
    
    tokens = loaded_inseq_data.sequence_attributions[0].target
    agent_out_filepath = Path(str(inseq_filepath).replace("_inseq_out.dill", "_agent_out.dill"))
    all_spans = {}

    all_spans.update(_get_predefined_spans(prompt_spans_data))
    all_spans.update(_find_spans_from_json_content(tokens, prompt_spans_data))
    all_spans.update(_find_function_spans(tokens, inseq_filepath, solutions_data, all_spans))
    all_spans.update(_find_agent_interaction_spans(tokens, agent_out_filepath))

    validation_passed, validation_message = _validate_spans(all_spans, len(tokens))
    return all_spans, validation_passed, validation_message

def save_solutions_with_compact_spans(data: list, output_path: Path):
    """Saves the solutions data to JSON with compact span lists."""
    indented_json_string = json.dumps(data, indent=4)
    compacted_json_string = re.sub(
        r'\[\s*(\d+),\s*(\d+)\s*\]', r'[\1, \2]', indented_json_string
    )
    try:
        with open(output_path, 'w') as f: f.write(compacted_json_string)
        print(f"\nSuccessfully saved updated span data to {output_path}")
    except Exception as e:
        print(f"\nError: Could not save data to {output_path}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Process agent output files in a specified directory to find, validate, and save token spans."
    )
    parser.add_argument(
        "directory",
        type=str,
        help="The specific category directory inside './outputs/' to process (e.g., 'finance', 'data')."
    )
    args = parser.parse_args()

    INPUT_DIRECTORY = Path(f"./outputs/{args.directory}")
    SOLUTIONS_JSON_PATH = Path("data/solutions.json")
    PROMPT_SPANS_JSON_PATH = Path("data/prompt_spans.json")
    
    REQUIRED_SPANS = {
        "agentic_behaviour_instructions", "few_shot_examples", "functions_intro",
        "final_answer_tool", "general_code_use_rules", "call_to_action",
        "correct_function", "decoy_functions_1", "decoy_functions_2",
        "user_prompt", "planning_step", "action_step"
    }

    if not INPUT_DIRECTORY.is_dir():
        print(f"Error: The specified directory does not exist: {INPUT_DIRECTORY.resolve()}")
        print("Please provide a valid category name.")
        exit()

    validation_failures = {}
    decoy_2_empty_files = []
    missing_spans_report = {}
    
    try:
        with open(SOLUTIONS_JSON_PATH, 'r') as f: solutions_data = json.load(f)
        with open(PROMPT_SPANS_JSON_PATH, 'r') as f: prompt_spans_data = json.load(f)
    except FileNotFoundError as e:
        print(f"Error: Could not find required data file {e.filename}. Exiting.")
        exit()

    prediction_map = {
        pred['file']: pred for record in solutions_data for pred in record.get("predictions", [])
    }

    files_to_process = list(INPUT_DIRECTORY.glob("*_inseq_out.dill"))
    if not files_to_process:
        print(f"No '_inseq_out.dill' files found in {INPUT_DIRECTORY.resolve()}.")
    else:
        for inseq_file in files_to_process:
            agent_out_name = inseq_file.name.replace("_inseq_out.dill", "_agent_out.dill")
            final_spans, validation_passed, validation_message = get_all_spans_from_file(inseq_file, prompt_spans_data, solutions_data)
            if final_spans is None: continue 
            found_spans = {name for name, span in final_spans.items() if span is not None}
            missing = REQUIRED_SPANS - found_spans
            if missing:
                missing_spans_report[inseq_file.name] = sorted(list(missing))
            if not validation_passed:
                validation_failures[inseq_file.name] = validation_message
            if final_spans.get("decoy_functions_2") is None:
                decoy_2_empty_files.append(inseq_file.name)
            if agent_out_name in prediction_map:
                spans_for_json = {k: list(v) for k, v in final_spans.items() if v is not None}
                prediction_map[agent_out_name]['spans'] = spans_for_json
            else:
                print(f"  - Warning: {agent_out_name} not found in solutions.json predictions.")

        print("\n" + "="*50)
        print("Batch Processing Summary")
        print("="*50)
        print(f"{len(files_to_process)} files were processed in directory: {INPUT_DIRECTORY.resolve()}")
        if not validation_failures:
            print("\nSuccess: All processed files' spans are contiguous and cover the entire token list perfectly.")
        else:
            print("\nThe following files' spans failed validation:")
            for filename, error_msg in validation_failures.items():
                print(f"  - {filename}: {error_msg}")
        if not decoy_2_empty_files:
            print("\nInfo: No files had an empty 'decoy_functions_2' span.")
        else:
            print("\nInfo: The following files had an empty 'decoy_functions_2' span:")
            print(decoy_2_empty_files)
        if not missing_spans_report:
            print("\nInfo: All processed files contained all required spans.")
        else:
            print("\nWarning: The following files were missing one or more required spans:")
            for filename, missing_list in missing_spans_report.items():
                print(f"  - File \"{filename}\" is missing the spans {missing_list}")
        save_solutions_with_compact_spans(solutions_data, SOLUTIONS_JSON_PATH)